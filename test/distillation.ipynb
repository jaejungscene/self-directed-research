{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of model parameters: 63,608,392\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "model = timm.create_model('vit_small_resnet26d_224')\n",
    "print('the number of model parameters: {:,}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of model parameters: 41,116,772\n",
      "torch.Size([2, 3, 256, 256])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ljj0512/private/project/test/distillation.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcilab1.ajou.ac.kr/home/ljj0512/private/project/test/distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(img\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcilab1.ajou.ac.kr/home/ljj0512/private/project/test/distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(labels\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcilab1.ajou.ac.kr/home/ljj0512/private/project/test/distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m loss \u001b[39m=\u001b[39m distiller(img, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcilab1.ajou.ac.kr/home/ljj0512/private/project/test/distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcilab1.ajou.ac.kr/home/ljj0512/private/project/test/distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# after lots of training above ...\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/vit_pytorch/distill.py:143\u001b[0m, in \u001b[0;36mDistillWrapper.forward\u001b[0;34m(self, img, labels, temperature, alpha, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(student_logits, labels)\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhard:\n\u001b[0;32m--> 143\u001b[0m     distill_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mkl_div(\n\u001b[1;32m    144\u001b[0m         F\u001b[39m.\u001b[39;49mlog_softmax(distill_logits \u001b[39m/\u001b[39;49m T, dim \u001b[39m=\u001b[39;49m \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    145\u001b[0m         F\u001b[39m.\u001b[39;49msoftmax(teacher_logits \u001b[39m/\u001b[39;49m T, dim \u001b[39m=\u001b[39;49m \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mdetach(),\n\u001b[1;32m    146\u001b[0m     reduction \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mbatchmean\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    147\u001b[0m     distill_loss \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m T \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py:2916\u001b[0m, in \u001b[0;36mkl_div\u001b[0;34m(input, target, size_average, reduce, reduction, log_target)\u001b[0m\n\u001b[1;32m   2913\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2914\u001b[0m         reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m-> 2916\u001b[0m reduced \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mkl_div(\u001b[39minput\u001b[39;49m, target, reduction_enum, log_target\u001b[39m=\u001b[39;49mlog_target)\n\u001b[1;32m   2918\u001b[0m \u001b[39mif\u001b[39;00m reduction \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatchmean\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2919\u001b[0m     reduced \u001b[39m=\u001b[39m reduced \u001b[39m/\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1000) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "from vit_pytorch.distill import DistillableViT, DistillWrapper\n",
    "\n",
    "teacher = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "model = DistillableViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 100,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "# print()\n",
    "print('the number of model parameters: {:,}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "distiller = DistillWrapper(\n",
    "    student = model,\n",
    "    teacher = teacher,\n",
    "    temperature = 3,           # temperature of distillation\n",
    "    alpha = 0.5,               # trade between main loss and distillation loss\n",
    "    hard = False               # whether to use soft or hard distillation\n",
    ")\n",
    "\n",
    "img = torch.randn(2, 3, 256, 256)\n",
    "labels = torch.randint(0, 100, (2,))\n",
    "print(img.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "loss = distiller(img, labels)\n",
    "loss.backward()\n",
    "\n",
    "# after lots of training above ...\n",
    "\n",
    "pred = model(img) # (2, 1000)\n",
    "print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65fe116ec29312474b580f4ecbad52a94f46ea3a142b15e85ff8e68848a207e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
